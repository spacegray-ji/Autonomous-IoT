{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f7b0b28",
   "metadata": {},
   "source": [
    "# VoteNet\n",
    "\n",
    "- Utilities\n",
    "1. Radius Search\n",
    "2. IoU for axis-aligned 3D bounding boxes\n",
    "3. 3D non-maximum suppression(NMS)\n",
    "- Modules\n",
    "1. Voting module\n",
    "2. Detection head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d369c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46406f4c",
   "metadata": {},
   "source": [
    "## Useful utilities and modules we already implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9d5d795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 16])\n"
     ]
    }
   ],
   "source": [
    "def find_knn_general(query_points, dataset_points, k):\n",
    "    M = len(query_points)\n",
    "    N = len(dataset_points)\n",
    "    \n",
    "    # 1. Compute pairwise distance\n",
    "    delta = query_points.view(M, 1, 3) - dataset_points.view(1, N, 3) # (M, N, 3)\n",
    "    dist = torch.sum(delta ** 2, dim=-1) # (M, N)\n",
    "    \n",
    "    # 2. Find k-nearest neighbor indices and corresponding features\n",
    "    knn_dist, knn_indices = dist.topk(k=k, dim=-1, largest=False) # (M, k)\n",
    "    \n",
    "    return knn_dist, knn_indices\n",
    "\n",
    "\n",
    "def interpolate_knn(query_points, dataset_points, dataset_features, k):\n",
    "    M = len(query_points)\n",
    "    N, C = dataset_features.shape\n",
    "    \n",
    "    # 1. Find k-nearest neighbor indices and corresponding features\n",
    "    knn_dist, knn_indices = find_knn_general(query_points, dataset_points, k)\n",
    "    knn_dataset_features = dataset_features[knn_indices.view(-1)].view(M, k, C)\n",
    "    \n",
    "    # 3. Calculate interpolation wegihts\n",
    "    knn_dist_recip = 1. / (knn_dist + 1e-8) # (M, k)\n",
    "    denom = knn_dist_recip.sum(dim=-1, keepdim=True) # (M, 1)\n",
    "    weights = knn_dist_recip / denom # (M, k)\n",
    "    \n",
    "    # 4. Linear interpolation\n",
    "    weighted_features = weights.view(M, k, 1) * knn_dataset_features # (M, k, C)\n",
    "    interpolated_features = weighted_features.sum(dim=1) # (M, C)\n",
    "    \n",
    "    return interpolated_features\n",
    "\n",
    "\n",
    "def farthest_point_sampling(points, num_samples):\n",
    "    N = len(points)\n",
    "    \n",
    "    # 1. Initialization\n",
    "    sampled_indices = torch.zeros(num_samples, dtype=torch.long)\n",
    "    distance = torch.ones(N,) * 1e10\n",
    "    farthest_idx = random.randint(0, N)\n",
    "    \n",
    "    # 2. Iteratively sample the farthest points\n",
    "    for i in range(num_samples):\n",
    "        # 2-1. Sample the farthest point\n",
    "        sampled_indices[i] = farthest_idx\n",
    "        \n",
    "        # 2-2. Compute distances between the sampled point and other (remaining) points\n",
    "        centroid = points[farthest_idx].view(1, 3)\n",
    "        delta = points - centroid\n",
    "        dist = torch.sum(delta ** 2, dim=-1) # (N,)\n",
    "        mask = dist < distance\n",
    "        distance[mask] = dist[mask]\n",
    "        \n",
    "        # 2-3. Sample the next farthest point\n",
    "        farthest_idx = torch.max(distance, -1)[1]\n",
    "\n",
    "    return sampled_indices\n",
    "\n",
    "\n",
    "class PointTransformerLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, k):\n",
    "        super(PointTransformerLayer, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.k = k\n",
    "        \n",
    "        self.linear_q = nn.Linear(in_channels, out_channels, bias=False)\n",
    "        self.linear_k = nn.Linear(in_channels, out_channels, bias=False)\n",
    "        self.linear_v = nn.Linear(in_channels, out_channels, bias=False)\n",
    "        \n",
    "        self.mlp_attn = nn.Sequential(\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(out_channels, out_channels),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(out_channels, out_channels)\n",
    "        )\n",
    "        self.mlp_pos = nn.Sequential(\n",
    "            nn.Linear(3, 3),\n",
    "            nn.BatchNorm1d(3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(3, out_channels)\n",
    "        )\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, points, features):\n",
    "        N = len(points)\n",
    "        \n",
    "        # 1. Query, key, and value projections\n",
    "        f_q = self.linear_q(features) # (N, C_out)\n",
    "        f_k = self.linear_k(features) # (N, C_out)\n",
    "        f_v = self.linear_v(features) # (N, C_out)\n",
    "        \n",
    "        # 2. Find kNN for local self-attention\n",
    "        knn_dist, knn_indices = find_knn_general(points, points, self.k) # (N, k)\n",
    "        knn_points = points[knn_indices.view(-1)].view(N, self.k, 3)\n",
    "        knn_k = f_k[knn_indices.view(-1)].view(N, self.k, self.out_channels)\n",
    "        knn_v = f_v[knn_indices.view(-1)].view(N, self.k, self.out_channels)\n",
    "        \n",
    "        # 3. Calculate the relative positional encoding\n",
    "        rel_pos = points.view(N, 1, 3) - knn_points # (N, k, 3)\n",
    "        rel_pos_enc = self.mlp_pos(rel_pos.view(-1, 3)).view(N, self.k, -1) # (N, k, C_out)\n",
    "        \n",
    "        # 4. Vector similarity\n",
    "        vec_sim = f_q.view(N, 1, self.out_channels) - knn_k + rel_pos_enc\n",
    "        weights = self.mlp_attn(vec_sim.view(-1, self.out_channels)).view(N, self.k, self.out_channels)\n",
    "        weights = self.softmax(weights) # (N, k, C_out)\n",
    "        \n",
    "        # 5. Weighted sum\n",
    "        weighted_knn_v = weights * (knn_v + rel_pos_enc) # (N, k, C_out)\n",
    "        out_features = weighted_knn_v.sum(dim=1) # (N, C_out)\n",
    "        \n",
    "        return out_features\n",
    "    \n",
    "    \n",
    "class PointTransformerBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, channels, k):\n",
    "        super(PointTransformerBlock, self).__init__()\n",
    "        self.linear_in = nn.Linear(channels, channels)\n",
    "        self.pt_layer = PointTransformerLayer(channels, channels, k)\n",
    "        self.linear_out = nn.Linear(channels, channels)\n",
    "        \n",
    "    def forward(self, points, features):\n",
    "        out_features = self.linear_in(features)\n",
    "        out_features = self.pt_layer(points, out_features)\n",
    "        out_features = self.linear_out(out_features)\n",
    "        out_features += features\n",
    "        \n",
    "        return out_features\n",
    "    \n",
    "    \n",
    "class TransitionDown(nn.Module):\n",
    "    \n",
    "    def __init__(self, channels, ratio, k):\n",
    "        super(TransitionDown, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.ratio = ratio\n",
    "        self.k = k\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(channels, channels, bias=False),\n",
    "            nn.BatchNorm1d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels, channels, bias=False),\n",
    "            nn.BatchNorm1d(channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, points, features):\n",
    "        N = len(points)\n",
    "        M = int(N / self.ratio)\n",
    "        \n",
    "        # 1. Farthest point sampling\n",
    "        sampled_indices = farthest_point_sampling(points, M)\n",
    "        sampled_points = points[sampled_indices]\n",
    "        \n",
    "        # 2. kNN search\n",
    "        knn_dist, knn_indices = find_knn_general(sampled_points, points, self.k) # (M, K)\n",
    "        \n",
    "        # 3. MLP\n",
    "        knn_features = features[knn_indices.view(-1)] # (M*K, C)\n",
    "        out_knn_features = self.mlp(knn_features)\n",
    "        out_knn_features = out_knn_features.view(M, self.k, -1)\n",
    "        \n",
    "        # 4. Local max pooling\n",
    "        out_features = out_knn_features.max(dim=1)[0]\n",
    "        \n",
    "        return sampled_points, out_features\n",
    "    \n",
    "    \n",
    "class TransitionUp(nn.Module):\n",
    "    \n",
    "    def __init__(self, up_channels, down_channels, out_channels):\n",
    "        super(TransitionUp, self).__init__()\n",
    "        self.linear_up = nn.Linear(up_channels, out_channels)\n",
    "        self.linear_down = nn.Linear(down_channels, out_channels)\n",
    "        \n",
    "    def forward(self, up_points, up_features, down_points, down_features):\n",
    "        # 1. Feed-forward with the down linear layer\n",
    "        down_f = self.linear_down(down_features)\n",
    "        \n",
    "        # 2. Interpolation\n",
    "        interp_f = interpolate_knn(up_points, down_points, down_f, 3) # (N, C_out)\n",
    "        \n",
    "        # 3. Skip-connection\n",
    "        out_f = interp_f + self.linear_up(up_features)\n",
    "        \n",
    "        return out_f\n",
    "    \n",
    "    \n",
    "class SimplePointTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, ratio, k):\n",
    "        super(SimplePointTransformer, self).__init__()\n",
    "        self.layer = PointTransformerLayer(in_channels, out_channels, k)\n",
    "        self.down = TransitionDown(out_channels, ratio, k)\n",
    "        self.up = TransitionUp(out_channels, out_channels, out_channels)\n",
    "        \n",
    "    def forward(self, points, features):\n",
    "        skip_features = self.layer(points, features)\n",
    "        down_points, out_features = self.down(points, skip_features)\n",
    "        out_features = self.up(points, skip_features, down_points, out_features)\n",
    "        \n",
    "        return out_features\n",
    "    \n",
    "    \n",
    "############ Test ##############\n",
    "N = 100\n",
    "K = 5\n",
    "ratio = 4\n",
    "C_in = 3\n",
    "C_out = 16\n",
    "\n",
    "points = torch.randn(N, 3)\n",
    "features = torch.randn(N, C_in)\n",
    "net = SimplePointTransformer(C_in, C_out, ratio, K)\n",
    "\n",
    "out_features = net(points, features)\n",
    "print(out_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc71760",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1880e809",
   "metadata": {},
   "source": [
    "### 1. Radius Search\n",
    "- Input: dataset points (N, 3), query points (M, 3) and the radius, R\n",
    "- Output: indices, a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bcc987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_radius_general(query_points, dataset_points, r):\n",
    "    M = len(query_points)\n",
    "    N = len(dataset_points)\n",
    "    \n",
    "    # 1. Compute pairwise distance\n",
    "    delta = query_points.view(M, 1, 3) - dataset_points.view(1, N, 3) # (M, N, 3)\n",
    "    dist = torch.sum(delta ** 2, dim=-1) # (M, N)\n",
    "    \n",
    "    # 2. Find indices\n",
    "    mask = dist < r\n",
    "    indices = []\n",
    "    for mask_ in mask:\n",
    "        indices.append(torch.nonzero(mask_, as_tuple=True)[0])\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61dc0693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "tensor([ 3, 15, 37, 41, 63, 68, 77, 88, 92])\n"
     ]
    }
   ],
   "source": [
    "N = 20\n",
    "M = 100\n",
    "R = 0.6\n",
    "\n",
    "query_points = torch.randn(N, 3)\n",
    "dataset_points = torch.randn(M, 3)\n",
    "\n",
    "indices = find_radius_general(query_points, dataset_points, R)\n",
    "print(len(indices))\n",
    "print(indices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbdafed",
   "metadata": {},
   "source": [
    "### 2. IoU for Axis-aligned 3D Bounding Boxes\n",
    "- Input: a 3D bounding box (6,), the other set 3D bounding box (6,)\n",
    "- Output: IoU, a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaf9ac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_iou2d(bb1, bb2):\n",
    "    # bounding box: (x1, y1, x2, y2), (x1, y1): top left, (x2, y2): bottom right\n",
    "    # Coordinate system:\n",
    "    # (0., 0.) ... (0., 0.1) ... (0., 1.)\n",
    "    # (0.1, 0.), ...\n",
    "    #  ...\n",
    "    # (1., 0.), ...\n",
    "    \n",
    "    # 1. Find coordinates of the intersection rectangle.\n",
    "    x_left = max(bb1[0], bb2[0])\n",
    "    y_top = max(bb1[1], bb2[1])\n",
    "    x_right = min(bb1[2], bb2[2])\n",
    "    y_bottom = min(bb1[3], bb2[3])\n",
    "    \n",
    "    # 2. If there is no overlap, return 0. Otherwise, calculate the IoU.\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        iou = 0.\n",
    "    else:\n",
    "        intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "        bb1_area = (bb1[2] - bb1[0]) * (bb1[3] - bb1[1])\n",
    "        bb2_area = (bb2[2] - bb2[0]) * (bb2[3] - bb2[1])\n",
    "        iou = intersection_area / (bb1_area + bb2_area - intersection_area)\n",
    "        \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83a89352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1429)\n"
     ]
    }
   ],
   "source": [
    "bb1 = torch.Tensor([0.1, 0.1, 0.3, 0.3])\n",
    "bb2 = torch.Tensor([0.2, 0.2, 0.4, 0.4])\n",
    "iou = cal_iou2d(bb1, bb2) # should be 0.142857... (= 0.01 / (0.04 + 0.04 - 0.01))\n",
    "print(iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf5eee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_iou3d(bb1, bb2):\n",
    "    # bounding box: (x1, y1, z1, x2, y2, z2), z1 < z2\n",
    "    # Use the same coordinate system\n",
    "    \n",
    "    # 1. Find coordinates of the intersection cuboid.\n",
    "    x_small = max(bb1[0], bb2[0])\n",
    "    y_small = max(bb1[1], bb2[1])\n",
    "    z_small = max(bb1[2], bb2[2])\n",
    "    x_large = min(bb1[3], bb2[3])\n",
    "    y_large = min(bb1[4], bb2[4])\n",
    "    z_large = min(bb1[5], bb2[5])\n",
    "    \n",
    "    # 2. If there is no overlap, return 0. Otherwise, find the overlapped volume.\n",
    "    if x_large < x_small or y_large < y_small or z_large < z_small:\n",
    "        iou = 0.\n",
    "    else:\n",
    "        intersection_volume = (x_large - x_small) * (y_large - y_small) * (z_large - z_small)\n",
    "        bb1_volume = (bb1[3] - bb1[0]) * (bb1[4] - bb1[1]) * (bb1[5] - bb1[2])\n",
    "        bb2_volume = (bb2[3] - bb2[0]) * (bb2[4] - bb2[1]) * (bb2[5] - bb2[2])\n",
    "        iou = intersection_volume / (bb1_volume + bb2_volume - intersection_volume)\n",
    "        \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "865d0d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0667)\n"
     ]
    }
   ],
   "source": [
    "bb1 = torch.Tensor([0.1, 0.1, 0.1, 0.3, 0.3, 0.3])\n",
    "bb2 = torch.Tensor([0.2, 0.2, 0.2, 0.4, 0.4, 0.4])\n",
    "iou = cal_iou3d(bb1, bb2) # should be 0.066666... (= 0.001 / (0.008 + 0.008 - 0.001))\n",
    "print(iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1edde62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_iou3d_multi(box, boxes):\n",
    "    # box: (x1, y1, z1, x2, y2, z2), z1 < z2\n",
    "    # boxes: (N, 6)\n",
    "    # Use the same coordinate system\n",
    "    \n",
    "    # 1. Find coordinates of the intersection cuboid.\n",
    "    x_small = boxes[:, 0].clamp(min=box[0])\n",
    "    y_small = boxes[:, 1].clamp(min=box[1])\n",
    "    z_small = boxes[:, 2].clamp(min=box[2])\n",
    "    x_large = boxes[:, 3].clamp(max=box[3])\n",
    "    y_large = boxes[:, 4].clamp(max=box[4])\n",
    "    z_large = boxes[:, 5].clamp(max=box[5])\n",
    "    \n",
    "    # 2. Define the delta tensor.\n",
    "    x_delta = x_large - x_small\n",
    "    y_delta = y_large - y_small\n",
    "    z_delta = z_large - z_small\n",
    "    \n",
    "    # 3. Calculate IoUs.\n",
    "    iou = torch.zeros((len(boxes),), dtype=box.dtype)\n",
    "    has_overlap = (x_delta > 0) * (y_delta > 0) * (z_delta > 0)\n",
    "    \n",
    "    # 4. Find the overlapped volume if there is overlap.\n",
    "    if len(has_overlap.nonzero()) == 0:\n",
    "        return iou\n",
    "    else:\n",
    "        boxes_valid = boxes[has_overlap]\n",
    "        x_delta_valid = x_delta[has_overlap]\n",
    "        y_delta_valid = y_delta[has_overlap]\n",
    "        z_delta_valid = z_delta[has_overlap]\n",
    "\n",
    "        intersection_volume = x_delta_valid * y_delta_valid * z_delta_valid\n",
    "        box_volume = (box[3] - box[0]) * (box[4] - box[1]) * (box[5] - box[2])\n",
    "        boxes_volume = (boxes_valid[:, 3] - boxes_valid[:, 0]) \\\n",
    "                        * (boxes_valid[:, 4] - boxes_valid[:, 1]) \\\n",
    "                        * (boxes_valid[:, 5] - boxes_valid[:, 2])\n",
    "        iou_valid = intersection_volume / (box_volume + boxes_volume - intersection_volume)\n",
    "\n",
    "        iou[has_overlap] = iou_valid\n",
    "        \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96344a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0667, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "box = torch.tensor([0.1, 0.1, 0.1, 0.3, 0.3, 0.3])\n",
    "boxes = torch.tensor([[0.2, 0.2, 0.2, 0.4, 0.4, 0.4], [0.01, 0.01, 0.01, 0.03, 0.03, 0.03]])\n",
    "iou = cal_iou3d_multi(box, boxes) \n",
    "print(iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0029fe3",
   "metadata": {},
   "source": [
    "### 3. 3D Non-Maximum Suppression\n",
    "- Input: a set of bounding boxes (N, 6), the corresponding scores (N,), iou_threshold\n",
    "- Output: the output boxes (M, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6715b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(boxes, scores, threshold):\n",
    "    # 1. Sort boxes in the ascending order of scores.\n",
    "    order = scores.argsort()\n",
    "    \n",
    "    # 2. Iteratively perform NMS.\n",
    "    keep = []\n",
    "    while len(order) > 0:\n",
    "        # 2-1. Pick the box with the highest score among the remaining boxes.\n",
    "        idx = order[-1]\n",
    "        box = boxes[idx]\n",
    "        keep.append(box)\n",
    "        order = order[:-1]\n",
    "        \n",
    "        if len(order) == 0:\n",
    "            break\n",
    "            \n",
    "        # 2-2. Calculate IoU between the selected box and the others.\n",
    "        remaining_boxes = boxes[order]\n",
    "        iou = cal_iou3d_multi(box, remaining_boxes)\n",
    "        \n",
    "        # 2-3. Find the non-maximum boxes.\n",
    "        mask = iou < threshold\n",
    "        order = order[mask]\n",
    "        \n",
    "    return torch.stack(keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d9363b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1000, 0.1000, 0.1000, 0.3000, 0.3000, 0.3000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.4000, 0.4000, 0.4000]])\n"
     ]
    }
   ],
   "source": [
    "boxes = torch.tensor([\n",
    "    [0.1, 0.1, 0.1, 0.3, 0.3, 0.3],\n",
    "    [0.11, 0.11, 0.11, 0.31, 0.31, 0.31],\n",
    "    [0.2, 0.2, 0.2, 0.4, 0.4, 0.4],\n",
    "    [0.21, 0.21, 0.21, 0.41, 0.41, 0.41]\n",
    "])\n",
    "scores = torch.tensor([0.9, 0.8, 0.7, 0.6])\n",
    "nms_boxes = nms(boxes, scores, threshold=0.5)\n",
    "print(nms_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2b4ed3",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30d3357",
   "metadata": {},
   "source": [
    "### 1. Voting Module (including feature extraction)\n",
    "- Input: input points (N, 3), input features (N, C_in), output feature dimension (C_out), num_votes + ratio, K (for Point Transformer)\n",
    "- Output: the output votes (num_votes, 3), the output features (num_votes, C_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "577d1b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VotingModule(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, num_votes, ratio, k):\n",
    "        super(VotingModule, self).__init__()\n",
    "        self.num_votes = num_votes\n",
    "        \n",
    "        self.pfe = SimplePointTransformer(in_channels, out_channels, ratio, k) # Point Feature Extractor\n",
    "        self.voter = nn.Sequential(\n",
    "            nn.Linear(out_channels, out_channels, bias=False),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(out_channels, 3 + out_channels) # delta_x (3) and delta_f (C_out)\n",
    "        )\n",
    "        \n",
    "    def forward(self, points, features):\n",
    "        # 1. Point Feature Extraction (In our case, Point Transformer)\n",
    "        out_features = self.pfe(points, features)\n",
    "        \n",
    "        # 2. Sample seed points\n",
    "        indices = farthest_point_sampling(points, self.num_votes)\n",
    "        seed_points = points[indices]\n",
    "        seed_features = out_features[indices]\n",
    "        \n",
    "        # 3. Voting\n",
    "        residuals = self.voter(seed_features)\n",
    "        vote_points = seed_points + residuals[:, :3]\n",
    "        vote_features = seed_features + residuals[:, 3:]\n",
    "        \n",
    "        return vote_points, vote_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a48e2d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3])\n",
      "torch.Size([32, 16])\n"
     ]
    }
   ],
   "source": [
    "N = 50\n",
    "C_in = 3\n",
    "C_out = 16\n",
    "num_votes = 32\n",
    "ratio = 4\n",
    "K = 5\n",
    "\n",
    "points = torch.randn(N, 3)\n",
    "features = torch.randn(N, C_in)\n",
    "voting_m = VotingModule(C_in, C_out, num_votes, ratio, K)\n",
    "\n",
    "vote_points, vote_features = voting_m(points, features)\n",
    "print(vote_points.shape)\n",
    "print(vote_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8736891c",
   "metadata": {},
   "source": [
    "### 2. Detection Head\n",
    "- Input: vote_points (N, 3), vote_features(N, C_in), num_clusters, radius, nms_iou_threshold\n",
    "- Output: the detected bounding boxes (M, 1 + 6) # 1 (objectness) + 6 (box coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03d4f291",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, num_clusters, radius, nms_iou_threshold):\n",
    "        super(DetectionHead, self).__init__()\n",
    "        self.num_clusters = num_clusters\n",
    "        self.radius = radius\n",
    "        self.nms_iou_threshold = nms_iou_threshold\n",
    "        \n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Linear(3 + in_channels, in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels, in_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels, in_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.final = nn.Linear(in_channels, 7)\n",
    "        \n",
    "    def forward(self, vote_points, vote_features):\n",
    "        # 1. Sample cluster centroids.\n",
    "        sampled_indices = farthest_point_sampling(vote_points, self.num_clusters)\n",
    "        cluster_points = vote_points[sampled_indices]\n",
    "        \n",
    "        # 2. Find cluster neighbors.\n",
    "        indices = find_radius_general(cluster_points, vote_points, self.radius) # List[torch.LongTensor]\n",
    "        \n",
    "        # 3. Grouping (MLP1 and MLP2)\n",
    "        grouped_features = []\n",
    "        for group_center, group_indices in zip(cluster_points, indices):\n",
    "            # 3-1. Calculate the relative position.\n",
    "            features_in_group = vote_features[group_indices]\n",
    "            relative_pos = (group_center.unsqueeze(0) - vote_points[group_indices]) / self.radius\n",
    "            features_with_pos = torch.cat([relative_pos, features_in_group], dim=1)\n",
    "            \n",
    "            # 3-2. MLP1 -> MaxPool -> MLP2\n",
    "            group_feature = self.mlp1(features_with_pos).max(dim=0)[0]\n",
    "            group_feature = self.mlp2(group_feature)\n",
    "            grouped_features.append(group_feature)\n",
    "        grouped_features = torch.stack(grouped_features)\n",
    "        \n",
    "        # 4. Predict bounding boxes\n",
    "        boxes = self.final(grouped_features)\n",
    "        box_scores = boxes[:, 0].sigmoid()\n",
    "        box_coordinates = boxes[:, 1:]\n",
    "        \n",
    "        # 5. Non-maximum suppression\n",
    "        final_boxes = nms(box_coordinates, box_scores, self.nms_iou_threshold)\n",
    "        \n",
    "        return final_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a0b6e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 6])\n"
     ]
    }
   ],
   "source": [
    "N = 50\n",
    "C_in = 8\n",
    "num_clusters = 16\n",
    "radius = 0.9\n",
    "iou_threshold = 0.5\n",
    "\n",
    "vote_points = torch.randn(N, 3)\n",
    "vote_features = torch.randn(N, C_in)\n",
    "detection_h = DetectionHead(C_in, num_clusters, radius, iou_threshold)\n",
    "\n",
    "pred_boxes = detection_h(vote_points, vote_features)\n",
    "print(pred_boxes.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
