{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIURBYAKcsMd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "class VOCDataset(Dataset):\n",
        "\n",
        "    def __init__(self, is_train, image_dir, label_txt, image_size=448, grid_size=7, num_bboxes=2, num_classes=20):\n",
        "        self.is_train = is_train\n",
        "        self.image_size = image_size\n",
        "\n",
        "        self.S = grid_size\n",
        "        self.B = num_bboxes\n",
        "        self.C = num_classes\n",
        "\n",
        "        mean_rgb = [122.67891434, 116.66876762, 104.00698793]\n",
        "        self.mean = np.array(mean_rgb, dtype=np.float32)\n",
        "\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "\n",
        "        if isinstance(label_txt, list) or isinstance(label_txt, tuple):\n",
        "            # cat multiple list files together.\n",
        "            # This is useful for VOC2007/VOC2012 combination.\n",
        "            tmp_file = '/tmp/label.txt'\n",
        "            os.system('cat %s > %s' % (' '.join(label_txt), tmp_file))\n",
        "            label_txt = tmp_file\n",
        "\n",
        "        self.paths, self.boxes, self.labels = [], [], []\n",
        "\n",
        "        with open(label_txt) as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            splitted = line.strip().split()\n",
        "\n",
        "            fname = splitted[0]\n",
        "            path = os.path.join(image_dir, fname)\n",
        "            self.paths.append(path)\n",
        "\n",
        "            num_boxes = (len(splitted) - 1) // 5\n",
        "            box, label = [], []\n",
        "            for i in range(num_boxes):\n",
        "                x1 = float(splitted[5*i + 1])\n",
        "                y1 = float(splitted[5*i + 2])\n",
        "                x2 = float(splitted[5*i + 3])\n",
        "                y2 = float(splitted[5*i + 4])\n",
        "                c  =   int(splitted[5*i + 5])\n",
        "                box.append([x1, y1, x2, y2])\n",
        "                label.append(c)\n",
        "            self.boxes.append(torch.Tensor(box))\n",
        "            self.labels.append(torch.LongTensor(label))\n",
        "\n",
        "        self.num_samples = len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.paths[idx]\n",
        "        img = cv2.imread(path)\n",
        "        boxes = self.boxes[idx].clone() # [n, 4]\n",
        "        labels = self.labels[idx].clone() # [n,]\n",
        "\n",
        "        if self.is_train:\n",
        "            img, boxes = self.random_flip(img, boxes)\n",
        "            img, boxes = self.random_scale(img, boxes)\n",
        "\n",
        "            img = self.random_blur(img)\n",
        "            img = self.random_brightness(img)\n",
        "            img = self.random_hue(img)\n",
        "            img = self.random_saturation(img)\n",
        "\n",
        "            img, boxes, labels = self.random_shift(img, boxes, labels)\n",
        "            img, boxes, labels = self.random_crop(img, boxes, labels)\n",
        "\n",
        "        # For debug.\n",
        "        debug_dir = 'tmp/voc_tta'\n",
        "        os.makedirs(debug_dir, exist_ok=True)\n",
        "        img_show = img.copy()\n",
        "        box_show = boxes.numpy().reshape(-1)\n",
        "        n = len(box_show) // 4\n",
        "        for b in range(n):\n",
        "            pt1 = (int(box_show[4*b + 0]), int(box_show[4*b + 1]))\n",
        "            pt2 = (int(box_show[4*b + 2]), int(box_show[4*b + 3]))\n",
        "            cv2.rectangle(img_show, pt1=pt1, pt2=pt2, color=(0,255,0), thickness=1)\n",
        "        cv2.imwrite(os.path.join(debug_dir, 'test_{}.jpg'.format(idx)), img_show)\n",
        "\n",
        "        h, w, _ = img.shape\n",
        "        boxes /= torch.Tensor([[w, h, w, h]]).expand_as(boxes) # normalize (x1, y1, x2, y2) w.r.t. image width/height.\n",
        "        target = self.encode(boxes, labels) # [S, S, 5 x B + C]\n",
        "\n",
        "        img = cv2.resize(img, dsize=(self.image_size, self.image_size), interpolation=cv2.INTER_LINEAR)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # assuming the model is pretrained with RGB images.\n",
        "        img = (img - self.mean) / 255.0 # normalize from -1.0 to 1.0.\n",
        "        img = self.to_tensor(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def encode(self, boxes, labels):\n",
        "        \"\"\" Encode box coordinates and class labels as one target tensor.\n",
        "        Args:\n",
        "            boxes: (tensor) [[x1, y1, x2, y2]_obj1, ...], normalized from 0.0 to 1.0 w.r.t. image width/height.\n",
        "            labels: (tensor) [c_obj1, c_obj2, ...]\n",
        "        Returns:\n",
        "            An encoded tensor sized [S, S, 5 x B + C], 5=(x, y, w, h, conf)\n",
        "        \"\"\"\n",
        "\n",
        "        S, B, C = self.S, self.B, self.C\n",
        "        N = 5 * B + C\n",
        "\n",
        "        target = torch.zeros(S, S, N)\n",
        "        cell_size = 1.0 / float(S)\n",
        "        boxes_wh = boxes[:, 2:] - boxes[:, :2] # width and height for each box, [n, 2]\n",
        "        boxes_xy = (boxes[:, 2:] + boxes[:, :2]) / 2.0 # center x & y for each box, [n, 2]\n",
        "        for b in range(boxes.size(0)):\n",
        "            xy, wh, label = boxes_xy[b], boxes_wh[b], int(labels[b])\n",
        "\n",
        "            ij = (xy / cell_size).ceil() - 1.0\n",
        "            i, j = int(ij[0]), int(ij[1]) # y & x index which represents its location on the grid.\n",
        "            x0y0 = ij * cell_size # x & y of the cell left-top corner.\n",
        "            xy_normalized = (xy - x0y0) / cell_size # x & y of the box on the cell, normalized from 0.0 to 1.0.\n",
        "\n",
        "            # TBM, remove redundant dimensions from target tensor.\n",
        "            # To remove these, loss implementation also has to be modified.\n",
        "            for k in range(B):\n",
        "                s = 5 * k\n",
        "                target[j, i, s  :s+2] = xy_normalized\n",
        "                target[j, i, s+2:s+4] = wh\n",
        "                target[j, i, s+4    ] = 1.0\n",
        "            target[j, i, 5*B + label] = 1.0\n",
        "\n",
        "        return target\n",
        "\n",
        "    def random_flip(self, img, boxes):\n",
        "        if random.random() < 0.5:\n",
        "            return img, boxes\n",
        "\n",
        "        h, w, _ = img.shape\n",
        "\n",
        "        img = np.fliplr(img)\n",
        "\n",
        "        x1, x2 = boxes[:, 0], boxes[:, 2]\n",
        "        x1_new = w - x2\n",
        "        x2_new = w - x1\n",
        "        boxes[:, 0], boxes[:, 2] = x1_new, x2_new\n",
        "\n",
        "        return img, boxes\n",
        "\n",
        "    def random_scale(self, img, boxes):\n",
        "        if random.random() < 0.5:\n",
        "            return img, boxes\n",
        "\n",
        "        scale = random.uniform(0.8, 1.2)\n",
        "        h, w, _ = img.shape\n",
        "        img = cv2.resize(img, dsize=(int(w * scale), h), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "        scale_tensor = torch.FloatTensor([[scale, 1.0, scale, 1.0]]).expand_as(boxes)\n",
        "        boxes = boxes * scale_tensor\n",
        "\n",
        "        return img, boxes\n",
        "\n",
        "    def random_blur(self, bgr):\n",
        "        if random.random() < 0.5:\n",
        "            return bgr\n",
        "\n",
        "        ksize = random.choice([2, 3, 4, 5])\n",
        "        bgr = cv2.blur(bgr, (ksize, ksize))\n",
        "        return bgr\n",
        "\n",
        "    def random_brightness(self, bgr):\n",
        "        if random.random() < 0.5:\n",
        "            return bgr\n",
        "\n",
        "        hsv = cv2.cvtColor(bgr, cv2.COLOR_BGR2HSV)\n",
        "        h, s, v = cv2.split(hsv)\n",
        "        adjust = random.uniform(0.5, 1.5)\n",
        "        v = v * adjust\n",
        "        v = np.clip(v, 0, 255).astype(hsv.dtype)\n",
        "        hsv = cv2.merge((h, s, v))\n",
        "        bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
        "\n",
        "        return bgr\n",
        "\n",
        "    def random_hue(self, bgr):\n",
        "        if random.random() < 0.5:\n",
        "            return bgr\n",
        "\n",
        "        hsv = cv2.cvtColor(bgr, cv2.COLOR_BGR2HSV)\n",
        "        h, s, v = cv2.split(hsv)\n",
        "        adjust = random.uniform(0.8, 1.2)\n",
        "        h = h * adjust\n",
        "        h = np.clip(h, 0, 255).astype(hsv.dtype)\n",
        "        hsv = cv2.merge((h, s, v))\n",
        "        bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
        "\n",
        "        return bgr\n",
        "\n",
        "    def random_saturation(self, bgr):\n",
        "        if random.random() < 0.5:\n",
        "            return bgr\n",
        "\n",
        "        hsv = cv2.cvtColor(bgr, cv2.COLOR_BGR2HSV)\n",
        "        h, s, v = cv2.split(hsv)\n",
        "        adjust = random.uniform(0.5, 1.5)\n",
        "        s = s * adjust\n",
        "        s = np.clip(s, 0, 255).astype(hsv.dtype)\n",
        "        hsv = cv2.merge((h, s, v))\n",
        "        bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
        "\n",
        "        return bgr\n",
        "\n",
        "    def random_shift(self, img, boxes, labels):\n",
        "        if random.random() < 0.5:\n",
        "            return img, boxes, labels\n",
        "\n",
        "        center = (boxes[:, 2:] + boxes[:, :2]) / 2.0\n",
        "\n",
        "        h, w, c = img.shape\n",
        "        img_out = np.zeros((h, w, c), dtype=img.dtype)\n",
        "        mean_bgr = self.mean[::-1]\n",
        "        img_out[:, :] = mean_bgr\n",
        "\n",
        "        dx = random.uniform(-w*0.2, w*0.2)\n",
        "        dy = random.uniform(-h*0.2, h*0.2)\n",
        "        dx, dy = int(dx), int(dy)\n",
        "\n",
        "        if dx >= 0 and dy >= 0:\n",
        "            img_out[dy:, dx:] = img[:h-dy, :w-dx]\n",
        "        elif dx >= 0 and dy < 0:\n",
        "            img_out[:h+dy, dx:] = img[-dy:, :w-dx]\n",
        "        elif dx < 0 and dy >= 0:\n",
        "            img_out[dy:, :w+dx] = img[:h-dy, -dx:]\n",
        "        elif dx < 0 and dy < 0:\n",
        "            img_out[:h+dy, :w+dx] = img[-dy:, -dx:]\n",
        "\n",
        "        center = center + torch.FloatTensor([[dx, dy]]).expand_as(center) # [n, 2]\n",
        "        mask_x = (center[:, 0] >= 0) & (center[:, 0] < w) # [n,]\n",
        "        mask_y = (center[:, 1] >= 0) & (center[:, 1] < h) # [n,]\n",
        "        mask = (mask_x & mask_y).view(-1, 1) # [n, 1], mask for the boxes within the image after shift.\n",
        "\n",
        "        boxes_out = boxes[mask.expand_as(boxes)].view(-1, 4) # [m, 4]\n",
        "        if len(boxes_out) == 0:\n",
        "            return img, boxes, labels\n",
        "        shift = torch.FloatTensor([[dx, dy, dx, dy]]).expand_as(boxes_out) # [m, 4]\n",
        "\n",
        "        boxes_out = boxes_out + shift\n",
        "        boxes_out[:, 0] = boxes_out[:, 0].clamp_(min=0, max=w)\n",
        "        boxes_out[:, 2] = boxes_out[:, 2].clamp_(min=0, max=w)\n",
        "        boxes_out[:, 1] = boxes_out[:, 1].clamp_(min=0, max=h)\n",
        "        boxes_out[:, 3] = boxes_out[:, 3].clamp_(min=0, max=h)\n",
        "\n",
        "        labels_out = labels[mask.view(-1)]\n",
        "\n",
        "        return img_out, boxes_out, labels_out\n",
        "\n",
        "    def random_crop(self, img, boxes, labels):\n",
        "        if random.random() < 0.5:\n",
        "            return img, boxes, labels\n",
        "\n",
        "        center = (boxes[:, 2:] + boxes[:, :2]) / 2.0\n",
        "\n",
        "        h_orig, w_orig, _ = img.shape\n",
        "        h = random.uniform(0.6 * h_orig, h_orig)\n",
        "        w = random.uniform(0.6 * w_orig, w_orig)\n",
        "        y = random.uniform(0, h_orig - h)\n",
        "        x = random.uniform(0, w_orig - w)\n",
        "        h, w, x, y = int(h), int(w), int(x), int(y)\n",
        "\n",
        "        center = center - torch.FloatTensor([[x, y]]).expand_as(center) # [n, 2]\n",
        "        mask_x = (center[:, 0] >= 0) & (center[:, 0] < w) # [n,]\n",
        "        mask_y = (center[:, 1] >= 0) & (center[:, 1] < h) # [n,]\n",
        "        mask = (mask_x & mask_y).view(-1, 1) # [n, 1], mask for the boxes within the image after crop.\n",
        "\n",
        "        boxes_out = boxes[mask.expand_as(boxes)].view(-1, 4) # [m, 4]\n",
        "        if len(boxes_out) == 0:\n",
        "            return img, boxes, labels\n",
        "        shift = torch.FloatTensor([[x, y, x, y]]).expand_as(boxes_out) # [m, 4]\n",
        "\n",
        "        boxes_out = boxes_out - shift\n",
        "        boxes_out[:, 0] = boxes_out[:, 0].clamp_(min=0, max=w)\n",
        "        boxes_out[:, 2] = boxes_out[:, 2].clamp_(min=0, max=w)\n",
        "        boxes_out[:, 1] = boxes_out[:, 1].clamp_(min=0, max=h)\n",
        "        boxes_out[:, 3] = boxes_out[:, 3].clamp_(min=0, max=h)\n",
        "\n",
        "        labels_out = labels[mask.view(-1)]\n",
        "        img_out = img[y:y+h, x:x+w, :]\n",
        "\n",
        "        return img_out, boxes_out, labels_out\n",
        "\n",
        "\n",
        "def test():\n",
        "    from torch.utils.data import DataLoader\n",
        "\n",
        "    image_dir = 'data/VOC_allimgs/'\n",
        "    label_txt = ['data/voc2007.txt', 'data/voc2012.txt']\n",
        "\n",
        "    dataset = VOCDataset(True, image_dir, label_txt)\n",
        "    data_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
        "\n",
        "    data_iter = iter(data_loader)\n",
        "    for i in range(100):\n",
        "        img, target = next(data_iter)\n",
        "        print(img.size(), target.size())\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    test()\n"
      ]
    }
  ]
}