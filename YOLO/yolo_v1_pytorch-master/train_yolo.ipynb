{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install tensorboardX"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"umstljm35JhY","executionInfo":{"status":"ok","timestamp":1663392141480,"user_tz":-540,"elapsed":4427,"user":{"displayName":"김희원","userId":"13562236615599718077"}},"outputId":"2930e739-1f18-4967-c9f9-7913edb0092a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorboardX\n","  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n","\u001b[K     |████████████████████████████████| 125 kB 14.7 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.6)\n","Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1,>=3.8.0->tensorboardX) (1.15.0)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.5.1\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ZzQBYoJ4Ndv","executionInfo":{"status":"ok","timestamp":1663436328571,"user_tz":-540,"elapsed":15395,"user":{"displayName":"김희원","userId":"13562236615599718077"}},"outputId":"0a628962-0c85-4282-cf81-390e1e4cdb48"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["cd /content/drive/MyDrive/Colab_Notebooks/YOLO/yolo_v1_pytorch-master/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RkpMGmHf4N_q","executionInfo":{"status":"ok","timestamp":1663436329011,"user_tz":-540,"elapsed":446,"user":{"displayName":"김희원","userId":"13562236615599718077"}},"outputId":"5458399d-da53-4821-e84e-f50eebbefe86"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab_Notebooks/YOLO/yolo_v1_pytorch-master\n"]}]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import DataLoader\n","from torch.autograd import Variable\n","\n","from voc import VOCDataset\n","from darknet import DarkNet\n","from yolo_v1 import YOLOv1\n","from loss import Loss\n","\n","import os\n","import numpy as np\n","import math\n","from datetime import datetime\n","\n","from tensorboardX import SummaryWriter\n"],"metadata":{"id":"Uohjzcwe3697"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YH11AQ6bb6nb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663392169063,"user_tz":-540,"elapsed":2,"user":{"displayName":"김희원","userId":"13562236615599718077"}},"outputId":"6e739ac9-e209-45d3-d3f7-ce194ef4b71c"},"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA current_device: 0\n","CUDA device_count: 1\n"]}],"source":["\n","\n","\n","# Check if GPU devices are available.\n","use_gpu = torch.cuda.is_available()\n","assert use_gpu, 'Current implementation does not support CPU mode. Enable CUDA.'\n","print('CUDA current_device: {}'.format(torch.cuda.current_device()))\n","print('CUDA device_count: {}'.format(torch.cuda.device_count()))\n","\n","# Path to data dir.\n","image_dir = 'data/VOC_allimgs/'\n","\n","# Path to label files.\n","train_label = ('data/voc2007.txt', 'data/voc2012.txt')\n","val_label = 'data/voc2007test.txt'\n","\n","# Path to checkpoint file containing pre-trained DarkNet weight.\n","checkpoint_path = 'weights/darknet/model_best.pth.tar'\n","\n","# Frequency to print/log the results.\n","print_freq = 5\n","tb_log_freq = 5\n","\n","# Training hyper parameters.\n","init_lr = 0.001\n","base_lr = 0.01\n","momentum = 0.9\n","weight_decay = 5.0e-4\n","num_epochs = 135\n","batch_size = 64\n"]},{"cell_type":"code","source":["\n","# Learning rate scheduling.\n","def update_lr(optimizer, epoch, burnin_base, burnin_exp=4.0):\n","    if epoch == 0:\n","        lr = init_lr + (base_lr - init_lr) * math.pow(burnin_base, burnin_exp)\n","    elif epoch == 1:\n","        lr = base_lr\n","    elif epoch == 75:\n","        lr = 0.001\n","    elif epoch == 105:\n","        lr = 0.0001\n","    else:\n","        return\n","\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","\n","def get_lr(optimizer):\n","    for param_group in optimizer.param_groups:\n","        return param_group['lr']\n","\n","\n","\n"],"metadata":{"id":"YgJPOT_Z5U7o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define darknet\n","darknet = DarkNet(conv_only=True, bn=True, init_weight=True)\n","darknet.features = torch.nn.DataParallel(darknet.features)\n","\n","# Define YOLO model.\n","yolo = YOLOv1(darknet.features)\n","yolo.conv_layers = torch.nn.DataParallel(yolo.conv_layers)\n","yolo.cuda()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8z1Zum_o5ugb","executionInfo":{"status":"ok","timestamp":1663392659675,"user_tz":-540,"elapsed":2605,"user":{"displayName":"김희원","userId":"13562236615599718077"}},"outputId":"07556c0b-31b4-4fac-c5cd-af3aab5eec01"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["YOLOv1(\n","  (features): DataParallel(\n","    (module): Sequential(\n","      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n","      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n","      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","      (4): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (6): LeakyReLU(negative_slope=0.1, inplace=True)\n","      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","      (8): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n","      (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (10): LeakyReLU(negative_slope=0.1, inplace=True)\n","      (11): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (13): LeakyReLU(negative_slope=0.1, inplace=True)\n","      (14): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","      (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (16): LeakyReLU(negative_slope=0.1, inplace=True)\n","      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (19): LeakyReLU(negative_slope=0.1, inplace=True)\n","      (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","      (21): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","      (22): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (23): LeakyReLU(negative_slope=0.1, inplace=True)\n","      (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (26): LeakyReLU(negative_slope=0.1, inplace=True)\n","      (27): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","      (28): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (29): LeakyReLU(negative_slope=0.1, inplace=True)\n","      (30): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (32): LeakyReLU(negative_slope=0.1, inplace=True)\n","      (33): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","      (34): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (35): LeakyReLU(negative_slope=0.1, inplace=True)\n","      (36): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (38): LeakyReLU(negative_slope=0.1, inplace=True)\n","      (39): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","      (40): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (41): LeakyReLU(negative_slope=0.1, inplace=True)\n","      (42): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (43): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (44): LeakyReLU(negative_slope=0.1, inplace=True)\n","      (45): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n","      (46): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (47): LeakyReLU(negative_slope=0.1, inplace=True)\n","      (48): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (49): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (50): LeakyReLU(negative_slope=0.1, inplace=True)\n","      (51): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","      (52): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n","      (53): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (54): LeakyReLU(negative_slope=0.1, inplace=True)\n","      (55): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (56): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (57): LeakyReLU(negative_slope=0.1, inplace=True)\n","      (58): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n","      (59): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (60): LeakyReLU(negative_slope=0.1, inplace=True)\n","      (61): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (62): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (63): LeakyReLU(negative_slope=0.1, inplace=True)\n","    )\n","  )\n","  (conv_layers): DataParallel(\n","    (module): Sequential(\n","      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n","      (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (3): LeakyReLU(negative_slope=0.1)\n","      (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (5): LeakyReLU(negative_slope=0.1, inplace=True)\n","      (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (7): LeakyReLU(negative_slope=0.1, inplace=True)\n","    )\n","  )\n","  (fc_layers): Sequential(\n","    (0): Flatten()\n","    (1): Linear(in_features=50176, out_features=4096, bias=True)\n","    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n","    (3): Dropout(p=0.5, inplace=False)\n","    (4): Linear(in_features=4096, out_features=1470, bias=True)\n","    (5): Sigmoid()\n","  )\n",")"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# Load pre-trained darknet.\n","\n","src_state_dict = torch.load(checkpoint_path)['state_dict']\n","dst_state_dict = darknet.state_dict()\n","\n","for k in dst_state_dict.keys():\n","    print('Loading weight of', k)\n","    dst_state_dict[k] = src_state_dict[k]\n","darknet.load_state_dict(dst_state_dict)\n"],"metadata":{"id":"Ecg5brIT7RJH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Setup loss and optimizer.\n","criterion = Loss(feature_size=yolo.feature_size)\n","optimizer = torch.optim.SGD(yolo.parameters(), lr=init_lr, momentum=momentum, weight_decay=weight_decay)\n","\n","# Load Pascal-VOC dataset.\n","train_dataset = VOCDataset(True, image_dir, train_label)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","\n","val_dataset = VOCDataset(False, image_dir, val_label)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n","\n","print('Number of training images: ', len(train_dataset))\n","\n","# Open TensorBoardX summary writer\n","log_dir = datetime.now().strftime('%b%d_%H-%M-%S')\n","log_dir = os.path.join('results/yolo', log_dir)\n","writer = SummaryWriter(log_dir=log_dir)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QzqiC-OG5wn4","executionInfo":{"status":"ok","timestamp":1663392673277,"user_tz":-540,"elapsed":2628,"user":{"displayName":"김희원","userId":"13562236615599718077"}},"outputId":"47303d2a-b195-4af5-c04f-95acfaa2d615"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["Number of training images:  22129\n"]}]},{"cell_type":"code","source":["# Training loop.\n","logfile = open(os.path.join(log_dir, 'log.txt'), 'w')\n","best_val_loss = np.inf\n","\n","for epoch in range(num_epochs):\n","    print('\\n')\n","    print('Starting epoch {} / {}'.format(epoch, num_epochs))\n","\n","    # Training.\n","    yolo.train()\n","    total_loss = 0.0\n","    total_batch = 0\n","\n","    for i, (imgs, targets) in enumerate(train_loader):\n","        # Update learning rate.\n","        update_lr(optimizer, epoch, float(i) / float(len(train_loader) - 1))\n","        lr = get_lr(optimizer)\n","\n","        # Load data as a batch.\n","        batch_size_this_iter = imgs.size(0)\n","        imgs = Variable(imgs)\n","        targets = Variable(targets)\n","        imgs, targets = imgs.cuda(), targets.cuda()\n","\n","        # Forward to compute loss.\n","        preds = yolo(imgs)\n","        loss = criterion(preds, targets)\n","        loss_this_iter = loss.item()\n","        total_loss += loss_this_iter * batch_size_this_iter\n","        total_batch += batch_size_this_iter\n","\n","        # Backward to update model weight.\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Print current loss.\n","        if i % print_freq == 0:\n","            print('Epoch [%d/%d], Iter [%d/%d], LR: %.6f, Loss: %.4f, Average Loss: %.4f'\n","            % (epoch, num_epochs, i, len(train_loader), lr, loss_this_iter, total_loss / float(total_batch)))\n","\n","        # TensorBoard.\n","        n_iter = epoch * len(train_loader) + i\n","        if n_iter % tb_log_freq == 0:\n","            writer.add_scalar('train/loss', loss_this_iter, n_iter)\n","            writer.add_scalar('lr', lr, n_iter)\n","\n","    # Validation.\n","    yolo.eval()\n","    val_loss = 0.0\n","    total_batch = 0\n","\n","    for i, (imgs, targets) in enumerate(val_loader):\n","        # Load data as a batch.\n","        batch_size_this_iter = imgs.size(0)\n","        imgs = Variable(imgs)\n","        targets = Variable(targets)\n","        imgs, targets = imgs.cuda(), targets.cuda()\n","\n","        # Forward to compute validation loss.\n","        with torch.no_grad():\n","            preds = yolo(imgs)\n","        loss = criterion(preds, targets)\n","        loss_this_iter = loss.item()\n","        val_loss += loss_this_iter * batch_size_this_iter\n","        total_batch += batch_size_this_iter\n","    val_loss /= float(total_batch)\n","\n","    # Save results.\n","    logfile.writelines(str(epoch + 1) + '\\t' + str(val_loss) + '\\n')\n","    logfile.flush()\n","    torch.save(yolo.state_dict(), os.path.join(log_dir, 'model_latest.pth'))\n","    if best_val_loss > val_loss:\n","        best_val_loss = val_loss\n","        torch.save(yolo.state_dict(), os.path.join(log_dir, 'model_best.pth'))\n","\n","    # Print.\n","    print('Epoch [%d/%d], Val Loss: %.4f, Best Val Loss: %.4f'\n","    % (epoch + 1, num_epochs, val_loss, best_val_loss))\n","\n","    # TensorBoard.\n","    writer.add_scalar('test/loss', val_loss, epoch + 1)\n","\n","writer.close()\n","logfile.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":642},"id":"H96uDLZn5YGM","executionInfo":{"status":"error","timestamp":1663392681403,"user_tz":-540,"elapsed":537,"user":{"displayName":"김희원","userId":"13562236615599718077"}},"outputId":"a7d9e3e7-9726-4ece-9248-df460773243b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Starting epoch 0 / 135\n"]},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-90c82475dda3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtotal_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Update learning rate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mupdate_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/content/drive/MyDrive/Colab_Notebooks/YOLO/yolo_v1_pytorch-master/voc.py\", line 66, in __getitem__\n    img, boxes = self.random_flip(img, boxes)\n  File \"/content/drive/MyDrive/Colab_Notebooks/YOLO/yolo_v1_pytorch-master/voc.py\", line 142, in random_flip\n    h, w, _ = img.shape\nAttributeError: 'NoneType' object has no attribute 'shape'\n"]}]}]}